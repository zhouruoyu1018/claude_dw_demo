#!/usr/bin/env python3
"""
SQL è§£æå™¨ - æå–è¡€ç¼˜å’ŒæŒ‡æ ‡ä¿¡æ¯
"""
import re
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from pathlib import Path


@dataclass
class ColumnLineage:
    """å­—æ®µçº§è¡€ç¼˜"""
    target_column: str
    source_table: Optional[str] = None
    source_column: Optional[str] = None
    transform_type: str = 'DIRECT'  # DIRECT, SUM, COUNT, AVG, MAX, MIN, CASE, CUSTOM
    transform_expr: Optional[str] = None


@dataclass
class TableLineage:
    """è¡¨çº§è¡€ç¼˜"""
    source_table: str
    join_type: str = 'FROM'  # FROM, LEFT JOIN, INNER JOIN, RIGHT JOIN, FULL JOIN
    join_condition: Optional[str] = None


@dataclass
class Indicator:
    """æŒ‡æ ‡å®šä¹‰"""
    indicator_english_name: str
    indicator_name: str  # ä¸­æ–‡åï¼Œéœ€ç”¨æˆ·ç¡®è®¤
    calculation_logic: str
    standard_type: str  # é‡‘é¢/æ•°é‡/æ¯”ç‡
    indicator_category: str  # åŸå­æŒ‡æ ‡/æ´¾ç”ŸæŒ‡æ ‡/å¤åˆæŒ‡æ ‡
    data_type: str = 'DECIMAL'
    business_domain: str = ''  # ä»è¡¨åæ¨æ–­
    update_frequency: str = 'æ—¥'  # ä»åˆ†åŒºå­—æ®µæ¨æ–­


@dataclass
class SQLAnalysisResult:
    """SQL åˆ†æç»“æœ"""
    target_table: str
    sql_file: str
    table_lineage: List[TableLineage] = field(default_factory=list)
    column_lineage: List[ColumnLineage] = field(default_factory=list)
    indicators: List[Indicator] = field(default_factory=list)
    etl_logic_summary: str = ''


class SQLParser:
    """SQL è§£æå™¨"""

    def __init__(self, sql_content: str, sql_file: str = '', mcp_client=None):
        """
        Args:
            sql_content: SQL è„šæœ¬å†…å®¹
            sql_file: æ–‡ä»¶è·¯å¾„
            mcp_client: MCP å®¢æˆ·ç«¯ï¼ˆç”¨äºæŸ¥è¯¢å…ƒæ•°æ®ï¼‰
        """
        self.sql_content = self._normalize_sql(sql_content)
        self.sql_file = sql_file
        self.mcp_client = mcp_client
        self.result = SQLAnalysisResult(
            target_table='',
            sql_file=sql_file
        )
        self.target_columns_from_metadata = []  # ä»å…ƒæ•°æ®è·å–çš„çœŸå®å­—æ®µåˆ—è¡¨

    # èšåˆå‡½æ•°æ¨¡å¼
    AGG_PATTERNS = {
        'SUM': (r'SUM\s*\(\s*([^)]+)\s*\)', 'SUM'),
        'COUNT': (r'COUNT\s*\(\s*DISTINCT\s+([^)]+)\s*\)', 'COUNT'),
        'COUNT_ALL': (r'COUNT\s*\(\s*([^)]+)\s*\)', 'COUNT'),
        'AVG': (r'AVG\s*\(\s*([^)]+)\s*\)', 'AVG'),
        'MAX': (r'MAX\s*\(\s*([^)]+)\s*\)', 'MAX'),
        'MIN': (r'MIN\s*\(\s*([^)]+)\s*\)', 'MIN'),
    }

    # æ ‡å‡†ç±»å‹æ¨æ–­è§„åˆ™
    TYPE_INFERENCE = {
        r'amt|amount|money|price|fee': 'é‡‘é¢',
        r'cnt|count|num|quantity': 'æ•°é‡',
        r'rate|ratio|percent|pct': 'æ¯”ç‡',
        r'date|time|dt': 'æ—¶é—´',
        r'avg|mean': 'å¹³å‡å€¼',
    }

    # ä¸šåŠ¡åŸŸæ¨æ–­è§„åˆ™ï¼ˆä»è¡¨åï¼‰
    DOMAIN_INFERENCE = {
        r'loan': 'è´·æ¬¾',
        r'overdue|repay': 'è´·å',
        r'customer|user': 'å®¢æˆ·',
        r'product': 'äº§å“',
        r'risk': 'é£æ§',
    }

    # (å·²ç§»é™¤ï¼Œä¸Šé¢æœ‰æ–°çš„ __init__)

    @staticmethod
    def _normalize_sql(sql: str) -> str:
        """è§„èŒƒåŒ– SQLï¼šå»æ³¨é‡Šã€ç»Ÿä¸€ç©ºç™½ç¬¦"""
        # å»é™¤å•è¡Œæ³¨é‡Š
        sql = re.sub(r'--[^\n]*', '', sql)
        # å»é™¤å¤šè¡Œæ³¨é‡Š
        sql = re.sub(r'/\*.*?\*/', '', sql, flags=re.DOTALL)
        # ç»Ÿä¸€ç©ºç™½ç¬¦
        sql = re.sub(r'\s+', ' ', sql)
        return sql.strip()

    def parse(self) -> SQLAnalysisResult:
        """æ‰§è¡Œå®Œæ•´è§£ææµç¨‹"""
        self._extract_target_table()
        self._fetch_target_columns_from_metadata()  # âœ¨ æ–°å¢ï¼šæŸ¥è¯¢å…ƒæ•°æ®
        self._extract_table_lineage()
        self._extract_column_lineage()
        self._align_columns_with_metadata()  # âœ¨ æ–°å¢ï¼šå­—æ®µå¯¹é½
        self._identify_indicators()
        self._generate_summary()
        return self.result

    def _fetch_target_columns_from_metadata(self):
        """æŸ¥è¯¢ç›®æ ‡è¡¨çš„çœŸå®å­—æ®µåˆ—è¡¨ï¼ˆä»å…ƒæ•°æ®ï¼‰"""
        if not self.mcp_client or not self.result.target_table:
            return

        try:
            # è°ƒç”¨ MCP å·¥å…· list_columns
            columns_result = self.mcp_client.list_columns(self.result.target_table)
            if columns_result and 'columns' in columns_result:
                self.target_columns_from_metadata = [
                    col['column_name'] for col in columns_result['columns']
                    if col['column_name'] not in ('dt', 'create_time', 'update_time')  # æ’é™¤åˆ†åŒºå­—æ®µå’Œå…ƒä¿¡æ¯å­—æ®µ
                ]
        except Exception as e:
            # å…ƒæ•°æ®æŸ¥è¯¢å¤±è´¥ä¸å½±å“è§£ææµç¨‹
            print(f"âš ï¸ å…ƒæ•°æ®æŸ¥è¯¢å¤±è´¥: {e}")

    def _align_columns_with_metadata(self):
        """å­—æ®µå¯¹é½ï¼šSELECT åˆ«å â†’ ç›®æ ‡è¡¨çœŸå®å­—æ®µå"""
        if not self.target_columns_from_metadata:
            return

        # ç­–ç•¥ï¼šä½ç½®å¯¹é½ï¼ˆHive INSERT OVERWRITE é»˜è®¤æŒ‰ä½ç½®ï¼‰
        sql_columns = [cl.target_column for cl in self.result.column_lineage]

        # æ£€æŸ¥æ•°é‡æ˜¯å¦åŒ¹é…
        if len(sql_columns) != len(self.target_columns_from_metadata):
            print(f"âš ï¸ å­—æ®µæ•°é‡ä¸åŒ¹é…: SQL {len(sql_columns)} vs ç›®æ ‡è¡¨ {len(self.target_columns_from_metadata)}")
            print(f"   SQL å­—æ®µ: {sql_columns}")
            print(f"   ç›®æ ‡è¡¨å­—æ®µ: {self.target_columns_from_metadata}")
            # ä¸å¼ºåˆ¶ä¸­æ–­ï¼Œç»§ç»­ä½¿ç”¨ SQL ä¸­çš„åˆ«å
            return

        # æ‰§è¡Œä½ç½®å¯¹é½
        for i, col_lineage in enumerate(self.result.column_lineage):
            real_column_name = self.target_columns_from_metadata[i]
            if col_lineage.target_column != real_column_name:
                print(f"   ğŸ”€ å­—æ®µå¯¹é½: {col_lineage.target_column} â†’ {real_column_name}")
                col_lineage.target_column = real_column_name  # æ›¿æ¢ä¸ºçœŸå®å­—æ®µå

    def _extract_target_table(self):
        """æå–ç›®æ ‡è¡¨"""
        # Hive: INSERT OVERWRITE TABLE db.table PARTITION (dt='...')
        match = re.search(
            r'INSERT\s+OVERWRITE\s+TABLE\s+([a-z_][a-z0-9_.]*)',
            self.sql_content,
            re.I
        )
        if match:
            self.result.target_table = match.group(1)
            return

        # Impala/Doris: INSERT INTO db.table
        match = re.search(
            r'INSERT\s+INTO\s+([a-z_][a-z0-9_.]*)',
            self.sql_content,
            re.I
        )
        if match:
            self.result.target_table = match.group(1)

    def _extract_table_lineage(self):
        """æå–è¡¨çº§è¡€ç¼˜"""
        # 1. æå– FROM å­å¥
        from_match = re.search(
            r'FROM\s+([a-z_][a-z0-9_.]*)',
            self.sql_content,
            re.I
        )
        if from_match:
            self.result.table_lineage.append(
                TableLineage(
                    source_table=from_match.group(1),
                    join_type='FROM'
                )
            )

        # 2. æå– JOIN å­å¥
        join_pattern = r'(LEFT|RIGHT|INNER|FULL)?\s*JOIN\s+([a-z_][a-z0-9_.]*)\s+(?:AS\s+)?([a-z_]\w*)?\s+ON\s+([^JOIN|WHERE|GROUP|ORDER|LIMIT]+)'
        for match in re.finditer(join_pattern, self.sql_content, re.I):
            join_type_prefix = match.group(1) or 'INNER'
            table_name = match.group(2)
            join_condition = match.group(4).strip()

            self.result.table_lineage.append(
                TableLineage(
                    source_table=table_name,
                    join_type=f'{join_type_prefix} JOIN',
                    join_condition=join_condition
                )
            )

    def _extract_column_lineage(self):
        """æå–å­—æ®µçº§è¡€ç¼˜"""
        # æå– SELECT å­å¥
        select_match = re.search(
            r'SELECT\s+(.*?)\s+FROM',
            self.sql_content,
            re.I | re.DOTALL
        )
        if not select_match:
            return

        select_clause = select_match.group(1)

        # å¤„ç† SELECT * çš„æƒ…å†µ
        if re.match(r'^\s*\*\s*$', select_clause):
            # TODO: éœ€è¦è°ƒç”¨ MCP list_columns è·å–å®Œæ•´å­—æ®µåˆ—è¡¨
            return

        # åˆ†å‰²å­—æ®µè¡¨è¾¾å¼ï¼ˆé€—å·åˆ†éš”ï¼Œä½†è¦æ³¨æ„å‡½æ•°å†…çš„é€—å·ï¼‰
        fields = self._split_select_fields(select_clause)

        for field_expr in fields:
            field_expr = field_expr.strip()
            if not field_expr:
                continue

            # æå–åˆ«å
            alias_match = re.search(r'AS\s+([a-z_]\w*)\s*$', field_expr, re.I)
            if alias_match:
                target_column = alias_match.group(1)
                expr_part = field_expr[:alias_match.start()].strip()
            else:
                # æ²¡æœ‰ ASï¼Œå–æœ€åä¸€ä¸ªæ ‡è¯†ç¬¦
                tokens = re.findall(r'[a-z_]\w*', field_expr, re.I)
                target_column = tokens[-1] if tokens else ''
                expr_part = field_expr

            if not target_column:
                continue

            # åˆ†æè½¬æ¢ç±»å‹
            lineage = self._analyze_field_expression(target_column, expr_part)
            self.result.column_lineage.append(lineage)

    @staticmethod
    def _split_select_fields(select_clause: str) -> List[str]:
        """åˆ†å‰² SELECT å­—æ®µåˆ—è¡¨ï¼ˆå¤„ç†å‡½æ•°å†…çš„é€—å·ï¼‰"""
        fields = []
        current_field = ''
        paren_depth = 0

        for char in select_clause:
            if char == '(':
                paren_depth += 1
            elif char == ')':
                paren_depth -= 1
            elif char == ',' and paren_depth == 0:
                fields.append(current_field)
                current_field = ''
                continue
            current_field += char

        if current_field:
            fields.append(current_field)

        return fields

    def _analyze_field_expression(self, target_column: str, expr: str) -> ColumnLineage:
        """åˆ†æå­—æ®µè¡¨è¾¾å¼ï¼Œè¯†åˆ«è½¬æ¢ç±»å‹"""
        expr_upper = expr.upper()

        # 1. çª—å£å‡½æ•°æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼Œå› ä¸ºçª—å£å‡½æ•°å¯èƒ½åŒ…å«èšåˆï¼‰
        if 'OVER' in expr_upper and '(' in expr_upper:
            return ColumnLineage(
                target_column=target_column,
                transform_type='CUSTOM',
                transform_expr=expr
            )

        # 2. èšåˆå‡½æ•°æ£€æµ‹
        for agg_name, (pattern, transform_type) in self.AGG_PATTERNS.items():
            match = re.search(pattern, expr, re.I)
            if match:
                inner_expr = match.group(1).strip()
                source_info = self._extract_source_column(inner_expr)
                return ColumnLineage(
                    target_column=target_column,
                    source_table=source_info['table'],
                    source_column=source_info['column'],
                    transform_type=transform_type,
                    transform_expr=expr
                )

        # 3. CASE WHEN æ£€æµ‹
        if 'CASE' in expr_upper and 'WHEN' in expr_upper:
            return ColumnLineage(
                target_column=target_column,
                transform_type='CASE',
                transform_expr=expr
            )

        # 4. ç®—æœ¯è¿ç®—æ£€æµ‹
        if re.search(r'[+\-*/]', expr):
            return ColumnLineage(
                target_column=target_column,
                transform_type='CUSTOM',
                transform_expr=expr
            )

        # 4. ç›´æ¥æ˜ å°„
        source_info = self._extract_source_column(expr)
        return ColumnLineage(
            target_column=target_column,
            source_table=source_info['table'],
            source_column=source_info['column'],
            transform_type='DIRECT',
            transform_expr=None
        )

    @staticmethod
    def _extract_source_column(expr: str) -> Dict[str, Optional[str]]:
        """ä»è¡¨è¾¾å¼ä¸­æå–æºè¡¨å’Œæºå­—æ®µ"""
        # åŒ¹é… table_alias.column_name æˆ– column_name
        match = re.search(r'([a-z_]\w*)\.([a-z_]\w*)', expr, re.I)
        if match:
            return {'table': match.group(1), 'column': match.group(2)}

        # ä»…å­—æ®µå
        match = re.search(r'([a-z_]\w*)', expr, re.I)
        if match:
            return {'table': None, 'column': match.group(1)}

        return {'table': None, 'column': None}

    def _identify_indicators(self):
        """è¯†åˆ«è®¡ç®—æŒ‡æ ‡"""
        for col_lineage in self.result.column_lineage:
            # åªæœ‰èšåˆå­—æ®µæˆ–æ´¾ç”Ÿå­—æ®µæ‰ç®—æŒ‡æ ‡
            if col_lineage.transform_type in ('DIRECT',):
                continue

            # æ¨æ–­æ ‡å‡†ç±»å‹
            standard_type = self._infer_standard_type(col_lineage.target_column)

            # æ¨æ–­æŒ‡æ ‡ç±»åˆ«
            if col_lineage.transform_type in ('SUM', 'COUNT', 'AVG', 'MAX', 'MIN'):
                indicator_category = 'åŸå­æŒ‡æ ‡'
            elif col_lineage.transform_type in ('CASE', 'CUSTOM'):
                indicator_category = 'æ´¾ç”ŸæŒ‡æ ‡'
            else:
                indicator_category = 'å¤åˆæŒ‡æ ‡'

            # ç”Ÿæˆä¸­æ–‡åï¼ˆéœ€ç”¨æˆ·ç¡®è®¤ï¼‰
            indicator_name = self._generate_chinese_name(col_lineage.target_column)

            # æ¨æ–­ä¸šåŠ¡åŸŸ
            business_domain = self._infer_business_domain(self.result.target_table)

            indicator = Indicator(
                indicator_english_name=col_lineage.target_column,
                indicator_name=indicator_name,
                calculation_logic=col_lineage.transform_expr or col_lineage.target_column,
                standard_type=standard_type,
                indicator_category=indicator_category,
                business_domain=business_domain,
                data_type=self._infer_data_type(standard_type),
                update_frequency='æ—¥'  # é»˜è®¤æ—¥æ›´æ–°ï¼Œå¯æ ¹æ®åˆ†åŒºå­—æ®µè°ƒæ•´
            )
            self.result.indicators.append(indicator)

    def _infer_standard_type(self, column_name: str) -> str:
        """æ¨æ–­æ ‡å‡†ç±»å‹"""
        column_lower = column_name.lower()
        for pattern, type_name in self.TYPE_INFERENCE.items():
            if re.search(pattern, column_lower):
                return type_name
        return 'æ•°é‡'  # é»˜è®¤

    def _infer_business_domain(self, table_name: str) -> str:
        """æ¨æ–­ä¸šåŠ¡åŸŸ"""
        table_lower = table_name.lower()
        for pattern, domain_name in self.DOMAIN_INFERENCE.items():
            if re.search(pattern, table_lower):
                return domain_name
        return 'é€šç”¨'

    @staticmethod
    def _generate_chinese_name(english_name: str) -> str:
        """ç”Ÿæˆä¸­æ–‡åï¼ˆå ä½ï¼Œéœ€ç”¨æˆ·ç¡®è®¤ï¼‰"""
        # ç®€å•æ˜ å°„è§„åˆ™ï¼ˆå®é™…éœ€è¦è¯æ ¹è¡¨ï¼‰
        mapping = {
            'td': 'å½“æ—¥',
            'loan': 'æ”¾æ¬¾',
            'amt': 'é‡‘é¢',
            'cnt': 'ç¬”æ•°',
            'avg': 'å¹³å‡',
            'sum': 'æ€»',
            'max': 'æœ€å¤§',
            'min': 'æœ€å°',
        }

        parts = re.findall(r'[a-z]+', english_name.lower())
        chinese_parts = [mapping.get(part, f'[{part}]') for part in parts]
        return ''.join(chinese_parts)

    @staticmethod
    def _infer_data_type(standard_type: str) -> str:
        """æ¨æ–­æ•°æ®ç±»å‹"""
        if standard_type in ('é‡‘é¢', 'å¹³å‡å€¼'):
            return 'DECIMAL(20,2)'
        elif standard_type in ('æ•°é‡', 'æ¯”ç‡'):
            return 'BIGINT'
        else:
            return 'VARCHAR(255)'

    def _generate_summary(self):
        """ç”Ÿæˆ ETL é€»è¾‘æ‘˜è¦"""
        source_count = len(self.result.table_lineage)
        indicator_count = len(self.result.indicators)

        if source_count == 1:
            summary = f"ä» {self.result.table_lineage[0].source_table} å•è¡¨åŠ å·¥"
        else:
            summary = f"å…³è” {source_count} å¼ æºè¡¨åŠ å·¥"

        if indicator_count > 0:
            summary += f"ï¼Œè®¡ç®— {indicator_count} ä¸ªèšåˆæŒ‡æ ‡"

        self.result.etl_logic_summary = summary


def parse_sql_file(file_path: str) -> SQLAnalysisResult:
    """è§£æå•ä¸ª SQL æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        sql_content = f.read()

    parser = SQLParser(sql_content, file_path)
    return parser.parse()


def batch_parse_sql_files(file_paths: List[str]) -> List[SQLAnalysisResult]:
    """æ‰¹é‡è§£æ SQL æ–‡ä»¶"""
    results = []
    for file_path in file_paths:
        try:
            result = parse_sql_file(file_path)
            results.append(result)
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {file_path} - {e}")
    return results


if __name__ == '__main__':
    # æµ‹è¯•ç”¨ä¾‹
    test_sql = """
    INSERT OVERWRITE TABLE dm.dmm_sac_loan_prod_daily PARTITION (dt='${hivevar:dt}')
    SELECT
        a.dt,
        a.product_id,
        b.product_name,
        SUM(a.loan_amount) AS td_loan_amt,
        COUNT(DISTINCT a.loan_id) AS td_loan_cnt,
        SUM(CASE WHEN a.loan_status = 'SUCCESS' THEN 1 ELSE 0 END) AS success_cnt,
        SUM(a.loan_amount) / NULLIF(COUNT(DISTINCT a.loan_id), 0) AS avg_loan_amt
    FROM dwd.dwd_loan_detail a
    LEFT JOIN dwd.dwd_product_info b ON a.product_id = b.product_id
    WHERE a.dt = '${hivevar:dt}'
    GROUP BY a.dt, a.product_id, b.product_name
    """

    parser = SQLParser(test_sql, 'test.sql')
    result = parser.parse()

    print(f"ç›®æ ‡è¡¨: {result.target_table}")
    print(f"\nè¡¨çº§è¡€ç¼˜:")
    for tl in result.table_lineage:
        print(f"  - {tl.source_table} ({tl.join_type})")

    print(f"\nå­—æ®µçº§è¡€ç¼˜:")
    for cl in result.column_lineage:
        print(f"  - {cl.target_column} â† {cl.source_table}.{cl.source_column} [{cl.transform_type}]")

    print(f"\nè¯†åˆ«çš„æŒ‡æ ‡:")
    for ind in result.indicators:
        print(f"  - {ind.indicator_english_name} ({ind.indicator_name}) - {ind.standard_type}")

    print(f"\nETL é€»è¾‘æ‘˜è¦: {result.etl_logic_summary}")
